{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn.python.ops import core_rnn_cell\n",
    "from tensorflow.python.ops import rnn_cell_impl\n",
    "\n",
    "from utils import createVocabulary\n",
    "from utils import loadVocabulary\n",
    "from utils import computeF1Score\n",
    "from utils import DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(allow_abbrev=False)\n",
    "\n",
    "#Network\n",
    "parser.add_argument(\"--num_units\", type=int, default=64, help=\"Network size.\", dest='layer_size')\n",
    "parser.add_argument(\"--model_type\", type=str, default='full', help=\"\"\"full(default) | intent_only\n",
    "                                                                    full: full attention model\n",
    "                                                                    intent_only: intent attention model\"\"\")\n",
    "\n",
    "#Training Environment\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16, help=\"Batch size.\")\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=20, help=\"Max epochs to train.\")\n",
    "parser.add_argument(\"--no_early_stop\", action='store_false',dest='early_stop', help=\"Disable early stop, which is based on sentence level accuracy.\")\n",
    "parser.add_argument(\"--patience\", type=int, default=5, help=\"Patience to wait before stop.\")\n",
    "\n",
    "#Model and Vocab\n",
    "parser.add_argument(\"--dataset\", type=str, default=None, help=\"\"\"Type 'atis' or 'snips' to use dataset provided by us or enter what ever you named your own dataset.\n",
    "                Note, if you don't want to use this part, enter --dataset=''. It can not be None\"\"\")\n",
    "parser.add_argument(\"--model_path\", type=str, default='./model', help=\"Path to save model.\")\n",
    "parser.add_argument(\"--vocab_path\", type=str, default='./vocab', help=\"Path to vocabulary files.\")\n",
    "\n",
    "#Data\n",
    "parser.add_argument(\"--train_data_path\", type=str, default='train', help=\"Path to training data files.\")\n",
    "parser.add_argument(\"--test_data_path\", type=str, default='test', help=\"Path to testing data files.\")\n",
    "parser.add_argument(\"--valid_data_path\", type=str, default='valid', help=\"Path to validation data files.\")\n",
    "parser.add_argument(\"--input_file\", type=str, default='seq.in', help=\"Input file name.\")\n",
    "parser.add_argument(\"--slot_file\", type=str, default='seq.out', help=\"Slot file name.\")\n",
    "parser.add_argument(\"--intent_file\", type=str, default='label', help=\"Intent file name.\")\n",
    "\n",
    "arg=parser.parse_args()\n",
    "\n",
    "#Print arguments\n",
    "for k,v in sorted(vars(arg).items()):\n",
    "    print(k,'=',v)\n",
    "print()\n",
    "\n",
    "if arg.model_type == 'full':\n",
    "    add_final_state_to_intent = True\n",
    "    remove_slot_attn = False\n",
    "elif arg.model_type == 'intent_only':\n",
    "    add_final_state_to_intent = True\n",
    "    remove_slot_attn = True\n",
    "else:\n",
    "    print('unknown model type!')\n",
    "    exit(1)\n",
    "\n",
    "#full path to data will be: ./data + dataset + train/test/valid\n",
    "if arg.dataset == None:\n",
    "    print('name of dataset can not be None')\n",
    "    exit(1)\n",
    "elif arg.dataset == 'snips':\n",
    "    print('use snips dataset')\n",
    "elif arg.dataset == 'atis':\n",
    "    print('use atis dataset')\n",
    "else:\n",
    "    print('use own dataset: ',arg.dataset)\n",
    "full_train_path = os.path.join('./data',arg.dataset,arg.train_data_path)\n",
    "full_test_path = os.path.join('./data',arg.dataset,arg.test_data_path)\n",
    "full_valid_path = os.path.join('./data',arg.dataset,arg.valid_data_path)\n",
    "\n",
    "print('12312312')\n",
    "createVocabulary(os.path.join(full_train_path, arg.input_file), os.path.join(arg.vocab_path, 'in_vocab'))\n",
    "createVocabulary(os.path.join(full_train_path, arg.slot_file), os.path.join(arg.vocab_path, 'slot_vocab'))\n",
    "createVocabulary(os.path.join(full_train_path, arg.intent_file), os.path.join(arg.vocab_path, 'intent_vocab'))\n",
    "\n",
    "in_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'in_vocab'))\n",
    "slot_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'slot_vocab'))\n",
    "intent_vocab = loadVocabulary(os.path.join(arg.vocab_path, 'intent_vocab'))\n",
    "\n",
    "print('in_vocab:\\n',in_vocab)\n",
    "print('slot_vocab:\\n',slot_vocab)\n",
    "print('intent_vocab:\\n',intent_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
